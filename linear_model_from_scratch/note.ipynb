{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "X = iris.data.to_numpy()  # type: ignore\n",
    "y = iris.target.to_numpy()  # type: ignore\n",
    "\n",
    "X = np.c_[np.ones(len(X)), X]  # With bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "valid_ratio = 0.2\n",
    "size = len(X)\n",
    "test_size = int(size * test_ratio)\n",
    "valid_size = int(size * valid_ratio)\n",
    "train_size = size - test_size - valid_size\n",
    "\n",
    "np.random.seed(42)\n",
    "ids = np.random.permutation(size)\n",
    "\n",
    "X_train = X[ids[:train_size]]\n",
    "y_train = y[ids[:train_size]]\n",
    "\n",
    "X_valid = X[ids[train_size:-test_size]]\n",
    "y_valid = y[ids[train_size:-test_size]]\n",
    "\n",
    "X_test = X[ids[-test_size:]]\n",
    "y_test = y[ids[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    return np.diag(np.ones(y.max() + 1))[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_one_hot(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_one_hot(y_train)\n",
    "Y_valid = to_one_hot(y_valid)\n",
    "Y_test = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train[:, 1:].mean(axis=0)  # Without bias term.\n",
    "std = X_train[:, 1:].mean(axis=0)\n",
    "\n",
    "X_train[:, 1:] = (X_train[:, 1:] - mean) / std\n",
    "X_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\n",
    "X_test[:, 1:] = (X_test[:, 1:] - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = exps.sum(axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 - Valid Loss: 2.1069489136021304\n",
      "Epoch: 100 - Valid Loss: 0.30879084881854263\n",
      "Epoch: 200 - Valid Loss: 0.24408622171429012\n",
      "Epoch: 300 - Valid Loss: 0.2180997345309118\n",
      "Epoch: 400 - Valid Loss: 0.203850681806548\n",
      "Epoch: 500 - Valid Loss: 0.1946614583857917\n",
      "Epoch: 600 - Valid Loss: 0.18809465830682998\n",
      "Epoch: 700 - Valid Loss: 0.18305240341590498\n",
      "Epoch: 800 - Valid Loss: 0.17896927278172073\n",
      "Epoch: 900 - Valid Loss: 0.17552567687462337\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "eps = 1e-9\n",
    "n_epochs = 1000\n",
    "m = len(X_train)\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    errors = softmax(X_train @ Theta) - Y_train\n",
    "    grads = 1 / m * X_train.T @ errors\n",
    "    Theta = Theta - eta * grads\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        Y_proba_valid = softmax(X_valid @ Theta)\n",
    "        xentropy_losses = -(Y_valid * np.log(np.clip(Y_proba_valid, eps, 1 - eps)))\n",
    "        mean_loss = xentropy_losses.sum(axis=1).mean()\n",
    "        print(f\"Epoch: {epoch:3d} - Valid Loss: {mean_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47265049,  3.07489532, -2.54140741],\n",
       "       [ 0.43242377, -0.04647024,  0.668786  ],\n",
       "       [ 3.33127519, -0.2266624 , -1.22743964],\n",
       "       [-5.27752905,  0.66376905,  4.2271726 ],\n",
       "       [-6.85188537, -2.37669375,  5.83234332]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = softmax(X_valid @ Theta)\n",
    "y_pred = Y_proba.argmax(axis=1)\n",
    "\n",
    "accuracy = (y_valid == y_pred).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 - Valid Loss: 2.10948\n",
      "Epoch: 100 - Valid Loss: 0.31671\n",
      "Epoch: 200 - Valid Loss: 0.25671\n",
      "Epoch: 300 - Valid Loss: 0.23449\n",
      "Epoch: 400 - Valid Loss: 0.22341\n",
      "Epoch: 500 - Valid Loss: 0.21697\n",
      "Epoch: 600 - Valid Loss: 0.21284\n",
      "Epoch: 700 - Valid Loss: 0.21000\n",
      "Epoch: 800 - Valid Loss: 0.20794\n",
      "Epoch: 900 - Valid Loss: 0.20635\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "eps = 1e-9\n",
    "alpha = 1e-2  # L2 regularization strength.\n",
    "n_epochs = 1000\n",
    "m = len(X_train)\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    errors = softmax(X_train @ Theta) - Y_train\n",
    "    grads = 1 / m * X_train.T @ errors\n",
    "    grads += np.r_[np.zeros((1, n_outputs)), alpha / m * Theta[1:]]  # Plus L2 term.\n",
    "    Theta = Theta - eta * grads\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        Y_proba_valid = softmax(X_valid @ Theta)\n",
    "        xentropy_loss = -(Y_valid * np.log(np.clip(Y_proba_valid, eps, 1 - eps)))\n",
    "        l2_loss = 2 * alpha / m * (Theta[1:] ** 2).sum()\n",
    "        total_loss = xentropy_loss.sum(axis=1).mean() + l2_loss\n",
    "        print(f\"Epoch: {epoch:3d} - Valid Loss: {total_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47765381,  3.01578941, -2.48730483],\n",
       "       [ 0.38368129, -0.04079102,  0.65484904],\n",
       "       [ 3.21336681, -0.24351742, -1.19412241],\n",
       "       [-5.15268403,  0.64795731,  4.13903126],\n",
       "       [-6.66748418, -2.29159497,  5.74638269]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = softmax(X_valid @ Theta)\n",
    "y_pred = Y_proba.argmax(axis=1)\n",
    "\n",
    "accuracy = (y_valid == y_pred).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 - Valid Loss: 2.47371\n",
      "Epoch: 100 - Valid Loss: 0.33325\n",
      "Epoch: 200 - Valid Loss: 0.28143\n",
      "Epoch: 300 - Valid Loss: 0.26592\n",
      "Epoch: 400 - Valid Loss: 0.26035\n",
      "Epoch: 500 - Valid Loss: 0.25854\n",
      "Epoch: 563 - Valid Loss: 0.25832 - Early Stopping\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "eps = 1e-9\n",
    "alpha = 3e-2  # L2 regularization strength.\n",
    "n_epochs = 1000\n",
    "m = len(X_train)\n",
    "best_loss = np.inf\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Y_train_proba = softmax(X_train @ Theta)\n",
    "    Y_valid_proba = softmax(X_valid @ Theta)\n",
    "    xentropy_loss = -(Y_valid * np.log(np.clip(Y_valid_proba, eps, 1 - eps)))\n",
    "    l2_loss = 2 * alpha / m * (Theta[1:] ** 2).sum()\n",
    "    total_loss = xentropy_loss.sum(axis=1).mean() + l2_loss\n",
    "\n",
    "    errors = Y_train_proba - Y_train\n",
    "    grads = 1 / m * X_train.T @ errors\n",
    "    grads += np.r_[np.zeros((1, n_outputs)), alpha / m * Theta[1:]]\n",
    "    Theta = Theta - eta * grads\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch:3d} - Valid Loss: {total_loss:.5f}\")\n",
    "\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "    else:\n",
    "        print(f\"Epoch: {epoch:3d} - Valid Loss: {total_loss:.5f} - Early Stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37983996,  2.48490524, -1.85860681],\n",
       "       [ 0.4652836 , -0.11644288,  0.61126296],\n",
       "       [ 2.82416639, -0.40012038, -0.71530102],\n",
       "       [-4.31627247,  0.71349348,  3.25087791],\n",
       "       [-5.68788957, -1.96749677,  4.56387565]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = softmax(X_valid @ Theta)\n",
    "y_pred = Y_proba.argmax(axis=1)\n",
    "\n",
    "accuracy = (y_valid == y_pred).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = softmax(X_test @ Theta)\n",
    "y_pred = Y_proba.argmax(axis=1)\n",
    "\n",
    "accuracy = (y_test == y_pred).mean()\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
