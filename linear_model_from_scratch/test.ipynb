{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    code {\n",
       "        background: rgba(42, 53, 125, 0.10) !important;\n",
       "        border-radius: 4px !important;\n",
       "    }\n",
       "    a {\n",
       "        color: rgba(123, 171, 237, 1.0) !important;\n",
       "    }\n",
       "    ol.numbered-list {\n",
       "    counter-reset: item;\n",
       "    }\n",
       "    ol.numbered-list li {\n",
       "    display: block;\n",
       "    }\n",
       "    ol.numbered-list li:before {\n",
       "    content: counters(item, '.') '. ';\n",
       "    counter-increment: item;\n",
       "    }\n",
       "    </style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load general_settings.py\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from array import array\n",
    "from collections import defaultdict, namedtuple\n",
    "from copy import copy\n",
    "from functools import partial, singledispatch\n",
    "from itertools import chain, combinations, product\n",
    "from pathlib import Path\n",
    "from time import strftime\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from colorama import Fore, Style\n",
    "from IPython.core.display import HTML, display_html\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.base import BaseEstimator, OneToOneFeatureMixin, TransformerMixin\n",
    "\n",
    "ON_KAGGLE = os.getenv(\"KAGGLE_KERNEL_RUN_TYPE\") is not None\n",
    "\n",
    "# Colorama settings.\n",
    "CLR = (Style.BRIGHT + Fore.BLACK) if ON_KAGGLE else (Style.BRIGHT + Fore.WHITE)\n",
    "RED = Style.BRIGHT + Fore.RED\n",
    "BLUE = Style.BRIGHT + Fore.BLUE\n",
    "CYAN = Style.BRIGHT + Fore.CYAN\n",
    "RESET = Style.RESET_ALL\n",
    "\n",
    "# Plots colors.\n",
    "FONT_COLOR = \"#4A4B52\"\n",
    "BACKGROUND_COLOR = \"#FFFCFA\"\n",
    "\n",
    "# Data Frame color theme.\n",
    "CELL_HOVER = {  # for row hover use <tr> instead of <td>\n",
    "    \"selector\": \"td:hover\",\n",
    "    \"props\": \"background-color: #FFFCFA\",\n",
    "}\n",
    "TEXT_HIGHLIGHT = {\n",
    "    \"selector\": \"td\",\n",
    "    \"props\": \"color: #4A4B52; font-weight: bold\",\n",
    "}\n",
    "INDEX_NAMES = {\n",
    "    \"selector\": \".index_name\",\n",
    "    \"props\": \"font-weight: normal; background-color: #FFFCFA; color: #4A4B52;\",\n",
    "}\n",
    "HEADERS = {\n",
    "    \"selector\": \"th:not(.index_name)\",\n",
    "    \"props\": \"font-weight: normal; background-color: #FFFCFA; color: #4A4B52;\",\n",
    "}\n",
    "DF_STYLE = (INDEX_NAMES, HEADERS, TEXT_HIGHLIGHT)\n",
    "DF_CMAP = sns.light_palette(\"#BAB8B8\", as_cmap=True)\n",
    "\n",
    "HTML_STYLE = \"\"\"\n",
    "    <style>\n",
    "    code {\n",
    "        background: rgba(42, 53, 125, 0.10) !important;\n",
    "        border-radius: 4px !important;\n",
    "    }\n",
    "    a {\n",
    "        color: rgba(123, 171, 237, 1.0) !important;\n",
    "    }\n",
    "    ol.numbered-list {\n",
    "    counter-reset: item;\n",
    "    }\n",
    "    ol.numbered-list li {\n",
    "    display: block;\n",
    "    }\n",
    "    ol.numbered-list li:before {\n",
    "    content: counters(item, '.') '. ';\n",
    "    counter-increment: item;\n",
    "    }\n",
    "    </style>\n",
    "\"\"\"\n",
    "\n",
    "# Utility functions.\n",
    "def download_from_kaggle(expr, directory=None, /) -> None:\n",
    "    if directory is None:\n",
    "        directory = Path(\"data\")\n",
    "    if not isinstance(directory, Path):\n",
    "        raise TypeError(\"The `directory` argument must be `Path` instance!\")\n",
    "    match expr:\n",
    "        case [\"kaggle\", _, \"download\", *args] if args:\n",
    "            directory.parent.mkdir(parents=True, exist_ok=True)\n",
    "            filename = args[-1].split(\"/\")[-1] + \".zip\"\n",
    "            if not (directory / filename).is_file():\n",
    "                subprocess.run(expr)\n",
    "                shutil.unpack_archive(filename, directory)\n",
    "                shutil.move(filename, directory)\n",
    "        case _:\n",
    "            raise SyntaxError(\"Invalid expression!\")\n",
    "\n",
    "\n",
    "def get_interpolated_colors(color1, color2, /, num_colors=2):\n",
    "    \"\"\"Return `num_colors` interpolated beetwen `color1` and `color2`.\n",
    "    Arguments need to be HEX.\"\"\"\n",
    "\n",
    "    def interpolate(color1, color2, t):\n",
    "        r1, g1, b1 = int(color1[1:3], 16), int(color1[3:5], 16), int(color1[5:7], 16)\n",
    "        r2, g2, b2 = int(color2[1:3], 16), int(color2[3:5], 16), int(color2[5:7], 16)\n",
    "        r = int(r1 + (r2 - r1) * t)\n",
    "        g = int(g1 + (g2 - g1) * t)\n",
    "        b = int(b1 + (b2 - b1) * t)\n",
    "        return f\"#{r:02X}{g:02X}{b:02X}\"\n",
    "\n",
    "    return [interpolate(color1, color2, k / (num_colors + 1)) for k in range(1, num_colors + 1)]\n",
    "\n",
    "\n",
    "def get_pretty_frame(frame, /, gradient=False, formatter=None, precision=3, repr_html=False):\n",
    "    stylish_frame = frame.style.set_table_styles(DF_STYLE).format(\n",
    "        formatter=formatter, precision=precision\n",
    "    )\n",
    "    if gradient:\n",
    "        stylish_frame = stylish_frame.background_gradient(DF_CMAP)  # type: ignore\n",
    "    if repr_html:\n",
    "        stylish_frame = stylish_frame.set_table_attributes(\"style='display:inline'\")._repr_html_()\n",
    "    return stylish_frame\n",
    "\n",
    "\n",
    "def numeric_descr(frame, /):\n",
    "    return (\n",
    "        frame.describe(percentiles=(0.01, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99))\n",
    "        .T.drop(\"count\", axis=1)\n",
    "        .rename(columns=str.title)\n",
    "    )\n",
    "\n",
    "\n",
    "def missing_unique_vals_summary(frame, /):\n",
    "    missing_vals = frame.isna().sum()\n",
    "    missing_vals_ratio = missing_vals / len(frame)\n",
    "    unique_vals = frame.apply(lambda col: len(col.unique()))\n",
    "    most_freq_count = frame.apply(lambda col: col.value_counts().iloc[0])\n",
    "    most_freq_val = frame.mode().iloc[:1].T.squeeze()\n",
    "    unique_ratio = unique_vals / len(frame)\n",
    "    freq_count_ratio = most_freq_count / len(frame)\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"Dtype\": frame.dtypes,\n",
    "            \"MissingValues\": missing_vals,\n",
    "            \"MissingValuesRatio\": missing_vals_ratio,\n",
    "            \"UniqueValues\": unique_vals,\n",
    "            \"UniqueValuesRatio\": unique_ratio,\n",
    "            \"MostFreqValue\": most_freq_val,\n",
    "            \"MostFreqValueCount\": most_freq_count,\n",
    "            \"MostFreqValueCountRatio\": freq_count_ratio,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def check_categories_alignment(frame1, frame2, /):\n",
    "    print(CLR + \"The same categories in training and test datasets?\\n\")\n",
    "    cat_features = frame2.select_dtypes(include=\"object\").columns.to_list()\n",
    "\n",
    "    for feature in cat_features:\n",
    "        frame1_unique = set(frame1[feature].unique())\n",
    "        frame2_unique = set(frame2[feature].unique())\n",
    "        same = np.all(frame1_unique == frame2_unique)\n",
    "        print(CLR + f\"{feature:25s}\", BLUE + f\"{same}\")\n",
    "\n",
    "\n",
    "def get_n_rows_and_axes(n_features, n_cols, /):\n",
    "    n_rows = int(np.ceil(n_features / n_cols))\n",
    "    current_col = range(1, n_cols + 1)\n",
    "    current_row = range(1, n_rows + 1)\n",
    "    return n_rows, list(product(current_row, current_col))\n",
    "\n",
    "\n",
    "def get_distributions_figure(feature_names, frame1, frame2, /, **kwargs):\n",
    "    histnorm = kwargs.get(\"histnorm\", \"probability density\")\n",
    "    train_color = kwargs.get(\"train_color\", \"blue\")\n",
    "    test_color = kwargs.get(\"test_color\", \"red\")\n",
    "    n_cols = kwargs.get(\"n_cols\", 3)\n",
    "    n_rows, axes = get_n_rows_and_axes(len(feature_names), n_cols)\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=n_rows,\n",
    "        cols=n_cols,\n",
    "        y_title=histnorm.title(),\n",
    "        horizontal_spacing=kwargs.get(\"horizontal_spacing\", 0.1),\n",
    "        vertical_spacing=kwargs.get(\"vertical_spacing\", 0.1),\n",
    "    )\n",
    "    fig.update_annotations(font_size=kwargs.get(\"annotations_font_size\", 14))\n",
    "\n",
    "    for frame, color, name in zip((frame1, frame2), (train_color, test_color), (\"Train\", \"Test\")):\n",
    "        if frame is None:  # Test dataset may not exist.\n",
    "            break\n",
    "\n",
    "        for k, (var, (row, col)) in enumerate(zip(feature_names, axes), start=1):\n",
    "            # density, bins = np.histogram(frame[var].dropna(), density=True)\n",
    "            fig.add_histogram(\n",
    "                x=frame[var],\n",
    "                histnorm=histnorm,\n",
    "                marker_color=color,\n",
    "                marker_line_width=0,\n",
    "                opacity=0.75,\n",
    "                name=name,\n",
    "                legendgroup=name,\n",
    "                showlegend=k == 1,\n",
    "                row=row,\n",
    "                col=col,\n",
    "            )\n",
    "            fig.update_xaxes(title_text=var, row=row, col=col)\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        tickfont_size=8, showgrid=False, titlefont_size=8, titlefont_family=\"Arial Black\"\n",
    "    )\n",
    "    fig.update_yaxes(tickfont_size=8, showgrid=False)\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=840,\n",
    "        height=kwargs.get(\"height\", 640),\n",
    "        title=kwargs.get(\"title\", \"Distributions\"),\n",
    "        font_color=FONT_COLOR,\n",
    "        title_font_size=18,\n",
    "        plot_bgcolor=BACKGROUND_COLOR,\n",
    "        paper_bgcolor=BACKGROUND_COLOR,\n",
    "        bargap=kwargs.get(\"bargap\", 0),\n",
    "        bargroupgap=kwargs.get(\"bargroupgap\", 0),\n",
    "        legend=dict(yanchor=\"bottom\", xanchor=\"right\", y=1, x=1, orientation=\"h\", title=\"\"),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Html highlight. Must be included at the end of all imports!\n",
    "HTML(HTML_STYLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "X = iris.data.to_numpy()  # type: ignore\n",
    "y = iris.target.to_numpy()  # type: ignore\n",
    "\n",
    "X = np.c_[np.ones(len(X)), X]  # With bias term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "valid_ratio = 0.2\n",
    "size = len(X)\n",
    "test_size = int(size * test_ratio)\n",
    "valid_size = int(size * valid_ratio)\n",
    "train_size = size - test_size - valid_size\n",
    "\n",
    "np.random.seed(42)\n",
    "ids = np.random.permutation(size)\n",
    "\n",
    "X_train = X[ids[:train_size]]\n",
    "y_train = y[ids[:train_size]]\n",
    "\n",
    "X_valid = X[ids[train_size:-test_size]]\n",
    "y_valid = y[ids[train_size:-test_size]]\n",
    "\n",
    "X_test = X[ids[-test_size:]]\n",
    "y_test = y[ids[-test_size:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    return np.diag(np.ones(y.max() + 1))[y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_one_hot(y_train[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = to_one_hot(y_train)\n",
    "Y_valid = to_one_hot(y_valid)\n",
    "Y_test = to_one_hot(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train[:, 1:].mean(axis=0)  # Without bias term.\n",
    "std = X_train[:, 1:].mean(axis=0)\n",
    "\n",
    "X_train[:, 1:] = (X_train[:, 1:] - mean) / std\n",
    "X_valid[:, 1:] = (X_valid[:, 1:] - mean) / std\n",
    "X_test[:, 1:] = (X_test[:, 1:] - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = exps.sum(axis=1, keepdims=True)\n",
    "    return exps / exp_sums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 - Valid Loss: 2.1069489136021304\n",
      "Epoch: 100 - Valid Loss: 0.30879084881854263\n",
      "Epoch: 200 - Valid Loss: 0.24408622171429012\n",
      "Epoch: 300 - Valid Loss: 0.2180997345309118\n",
      "Epoch: 400 - Valid Loss: 0.203850681806548\n",
      "Epoch: 500 - Valid Loss: 0.1946614583857917\n",
      "Epoch: 600 - Valid Loss: 0.18809465830682998\n",
      "Epoch: 700 - Valid Loss: 0.18305240341590498\n",
      "Epoch: 800 - Valid Loss: 0.17896927278172073\n",
      "Epoch: 900 - Valid Loss: 0.17552567687462337\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "eps = 1e-9\n",
    "n_epochs = 1000\n",
    "m = len(X_train)\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    errors = softmax(X_train @ Theta) - Y_train\n",
    "    grads = 1 / m * X_train.T @ errors\n",
    "    Theta = Theta - eta * grads\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        Y_proba_valid = softmax(X_valid @ Theta)\n",
    "        xentropy_losses = -(Y_valid * np.log(np.clip(Y_proba_valid, eps, 1 - eps)))\n",
    "        mean_loss = xentropy_losses.sum(axis=1).mean()\n",
    "        print(f\"Epoch: {epoch:3d} - Valid Loss: {mean_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47265049,  3.07489532, -2.54140741],\n",
       "       [ 0.43242377, -0.04647024,  0.668786  ],\n",
       "       [ 3.33127519, -0.2266624 , -1.22743964],\n",
       "       [-5.27752905,  0.66376905,  4.2271726 ],\n",
       "       [-6.85188537, -2.37669375,  5.83234332]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = softmax(X_valid @ Theta)\n",
    "y_pred = Y_proba.argmax(axis=1)\n",
    "\n",
    "accuracy = (y_valid == y_pred).mean()\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 - Valid Loss: 2.10948\n",
      "Epoch: 100 - Valid Loss: 0.31671\n",
      "Epoch: 200 - Valid Loss: 0.25671\n",
      "Epoch: 300 - Valid Loss: 0.23449\n",
      "Epoch: 400 - Valid Loss: 0.22341\n",
      "Epoch: 500 - Valid Loss: 0.21697\n",
      "Epoch: 600 - Valid Loss: 0.21284\n",
      "Epoch: 700 - Valid Loss: 0.21000\n",
      "Epoch: 800 - Valid Loss: 0.20794\n",
      "Epoch: 900 - Valid Loss: 0.20635\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "eps = 1e-9\n",
    "alpha = 1e-2  # L2 regularization strength.\n",
    "n_epochs = 1000\n",
    "m = len(X_train)\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    errors = softmax(X_train @ Theta) - Y_train\n",
    "    grads = 1 / m * X_train.T @ errors\n",
    "    grads += np.r_[np.zeros((1, n_outputs)), alpha / m * Theta[1:]]  # Plus L2 term.\n",
    "    Theta = Theta - eta * grads\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        Y_proba_valid = softmax(X_valid @ Theta)\n",
    "        xentropy_loss = -(Y_valid * np.log(np.clip(Y_proba_valid, eps, 1 - eps)))\n",
    "        l2_loss = 2 * alpha / m * (Theta[1:] ** 2).sum()\n",
    "        total_loss = xentropy_loss.sum(axis=1).mean() + l2_loss\n",
    "        print(f\"Epoch: {epoch:3d} - Valid Loss: {total_loss:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.47765381,  3.01578941, -2.48730483],\n",
       "       [ 0.38368129, -0.04079102,  0.65484904],\n",
       "       [ 3.21336681, -0.24351742, -1.19412241],\n",
       "       [-5.15268403,  0.64795731,  4.13903126],\n",
       "       [-6.66748418, -2.29159497,  5.74638269]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = softmax(X_valid @ Theta)\n",
    "y_pred = Y_proba.argmax(axis=1)\n",
    "\n",
    "accuracy = (y_valid == y_pred).mean()\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 - Valid Loss: 2.47371\n",
      "Epoch: 100 - Valid Loss: 0.33325\n",
      "Epoch: 200 - Valid Loss: 0.28143\n",
      "Epoch: 300 - Valid Loss: 0.26592\n",
      "Epoch: 400 - Valid Loss: 0.26035\n",
      "Epoch: 500 - Valid Loss: 0.25854\n",
      "Epoch: 563 - Valid Loss: 0.25832 - Early Stopping\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "eps = 1e-9\n",
    "alpha = 3e-2  # L2 regularization strength.\n",
    "n_epochs = 1000\n",
    "m = len(X_train)\n",
    "best_loss = np.inf\n",
    "\n",
    "np.random.seed(42)\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    Y_train_proba = softmax(X_train @ Theta)\n",
    "    Y_valid_proba = softmax(X_valid @ Theta)\n",
    "    xentropy_loss = -(Y_valid * np.log(np.clip(Y_valid_proba, eps, 1 - eps)))\n",
    "    l2_loss = 2 * alpha / m * (Theta[1:] ** 2).sum()\n",
    "    total_loss = xentropy_loss.sum(axis=1).mean() + l2_loss\n",
    "\n",
    "    errors = Y_train_proba - Y_train\n",
    "    grads = 1 / m * X_train.T @ errors\n",
    "    grads += np.r_[np.zeros((1, n_outputs)), alpha / m * Theta[1:]]\n",
    "    Theta = Theta - eta * grads\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch:3d} - Valid Loss: {total_loss:.5f}\")\n",
    "\n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "    else:\n",
    "        print(f\"Epoch: {epoch:3d} - Valid Loss: {total_loss:.5f} - Early Stopping\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37983996,  2.48490524, -1.85860681],\n",
       "       [ 0.4652836 , -0.11644288,  0.61126296],\n",
       "       [ 2.82416639, -0.40012038, -0.71530102],\n",
       "       [-4.31627247,  0.71349348,  3.25087791],\n",
       "       [-5.68788957, -1.96749677,  4.56387565]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = softmax(X_valid @ Theta)\n",
    "y_pred = Y_proba.argmax(axis=1)\n",
    "\n",
    "accuracy = (y_valid == y_pred).mean()\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_proba = softmax(X_test @ Theta)\n",
    "y_pred = Y_proba.argmax(axis=1)\n",
    "\n",
    "accuracy = (y_test == y_pred).mean()\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tensorflow_imports.py\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "K = keras.backend\n",
    "AUTO = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "X = tf.random.uniform((1, 224, 224, 3), seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 220, 220, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = layers.Conv2D(filters=64, kernel_size=5)\n",
    "conv(X).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 3, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.get_weights()[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultInceptionConv2D = partial(\n",
    "    layers.Conv2D,\n",
    "    strides=1,\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    ")\n",
    "\n",
    "\n",
    "class InceptionConv2D(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.conv = [\n",
    "            DefaultInceptionConv2D(filters=filters, kernel_size=kernel_size),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = inputs\n",
    "        for layer in self.conv:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class InceptionModule(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.concat = layers.Concatenate()\n",
    "        self.stack1 = [InceptionConv2D(filters=64)]\n",
    "        self.stack2 = [InceptionConv2D(filters=96), InceptionConv2D(filters=128, kernel_size=3)]\n",
    "        self.stack3 = [InceptionConv2D(filters=12), InceptionConv2D(filters=32, kernel_size=5)]\n",
    "        self.stack4 = [\n",
    "            layers.MaxPool2D(pool_size=3, strides=1, padding=\"same\"),\n",
    "            layers.BatchNormalization(),\n",
    "            InceptionConv2D(filters=32),\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X1, X2, X3, X4 = inputs, inputs, inputs, inputs\n",
    "        for layer in self.stack1:\n",
    "            X1 = layer(X1)\n",
    "        for layer in self.stack2:\n",
    "            X2 = layer(X2)\n",
    "        for layer in self.stack3:\n",
    "            X3 = layer(X3)\n",
    "        for layer in self.stack4:\n",
    "            X4 = layer(X4)\n",
    "        return self.concat([X1, X2, X3, X4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 224, 224, 3)\n",
      "(1, 224, 224, 256)\n"
     ]
    }
   ],
   "source": [
    "IM = InceptionModule()\n",
    "\n",
    "print(X.shape)\n",
    "print(IM(X).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 256), dtype=float32, numpy=\n",
       "array([[1.04704154e+00, 0.00000000e+00, 1.02873731e+00, 1.08971167e+00,\n",
       "        3.28178331e-02, 8.06148589e-01, 0.00000000e+00, 1.30096745e+00,\n",
       "        6.03694953e-02, 7.28407549e-03, 1.54893792e-06, 1.62363723e-02,\n",
       "        2.32358992e-01, 1.35494880e-02, 2.20537841e-01, 4.39143144e-02,\n",
       "        4.37835604e-03, 8.11324775e-01, 8.68748873e-02, 4.83216614e-01,\n",
       "        7.37776518e-01, 1.28926439e-02, 1.38001710e-01, 2.54548155e-03,\n",
       "        0.00000000e+00, 1.61289394e-01, 5.68381548e-01, 1.74389005e-01,\n",
       "        1.17599433e-02, 1.58854318e-03, 3.03267734e-03, 1.52783124e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.46079230e-01, 2.28877202e-01,\n",
       "        0.00000000e+00, 1.35189676e+00, 4.35059309e-01, 7.71518171e-01,\n",
       "        4.43152249e-01, 5.97636700e-01, 9.23033506e-02, 1.01532332e-05,\n",
       "        5.78622147e-03, 0.00000000e+00, 5.74101567e-01, 8.85211170e-01,\n",
       "        2.45120645e-01, 3.84467281e-02, 8.34716391e-03, 6.47285521e-01,\n",
       "        0.00000000e+00, 4.48955689e-03, 1.84527397e-01, 6.62742332e-02,\n",
       "        1.33415356e-01, 2.55502909e-02, 2.41250973e-02, 1.46708423e-02,\n",
       "        3.87318581e-01, 1.20636499e+00, 1.05891740e-02, 7.15404749e-01,\n",
       "        2.96969712e-02, 1.70730669e-02, 1.92052901e-01, 3.23139697e-01,\n",
       "        2.76331557e-04, 9.34092164e-01, 8.47643495e-01, 3.16705853e-01,\n",
       "        1.80242896e-01, 6.36786401e-01, 2.20785797e-01, 5.34846485e-02,\n",
       "        1.10476017e+00, 8.31309706e-02, 2.01074202e-02, 2.56712228e-01,\n",
       "        5.49323738e-01, 2.11315170e-01, 3.16003919e-01, 9.64484364e-03,\n",
       "        1.63620964e-01, 1.17507845e-01, 1.27942964e-01, 2.65048239e-02,\n",
       "        1.02188885e-01, 8.43761675e-03, 1.86627135e-02, 3.24227184e-01,\n",
       "        1.34516701e-01, 2.98024327e-01, 1.74893007e-01, 6.25751793e-01,\n",
       "        1.04062799e-02, 1.28236556e+00, 9.65079817e-05, 3.01175602e-02,\n",
       "        1.97674960e-01, 5.10991998e-02, 1.94614872e-01, 1.96380615e-01,\n",
       "        1.49785116e-01, 4.88260910e-02, 2.31531844e-01, 5.47066450e-01,\n",
       "        5.93885593e-02, 4.99079168e-01, 8.89891028e-01, 2.19774339e-02,\n",
       "        2.20358282e-01, 2.46109422e-02, 1.02122276e-05, 3.18228863e-02,\n",
       "        2.37635121e-01, 1.31989539e+00, 7.18442261e-01, 2.08544713e-02,\n",
       "        2.77684242e-01, 7.02639436e-03, 2.46700456e-05, 2.18077093e-01,\n",
       "        4.19235490e-02, 3.27960476e-02, 7.05178797e-01, 9.42876756e-01,\n",
       "        1.38315535e+00, 1.35855302e-01, 1.48173428e+00, 6.09369576e-01,\n",
       "        7.52170563e-01, 1.14439309e+00, 5.83831861e-04, 0.00000000e+00,\n",
       "        7.49093711e-01, 3.45075466e-02, 1.42285013e+00, 6.72599708e-04,\n",
       "        2.85391533e-03, 3.13576311e-01, 9.55891192e-01, 1.10473430e+00,\n",
       "        7.12894440e-01, 1.87673718e-01, 1.31555831e+00, 5.87608516e-01,\n",
       "        2.37055331e-01, 2.94706851e-01, 5.39831370e-02, 1.26233324e-01,\n",
       "        5.89620292e-01, 1.87784266e-02, 9.71677959e-01, 6.75961792e-01,\n",
       "        6.44923449e-01, 1.77750539e-03, 1.50913730e-01, 3.76841992e-01,\n",
       "        5.95230004e-03, 1.04905236e+00, 1.67409866e-03, 1.99208421e-06,\n",
       "        2.53192604e-01, 1.71727911e-02, 5.79404943e-02, 3.54795009e-02,\n",
       "        2.03672141e-01, 3.99569236e-03, 6.34014964e-01, 4.66853453e-06,\n",
       "        5.97352505e-01, 1.08447403e-01, 1.36939809e-02, 7.24882245e-01,\n",
       "        2.97845863e-02, 1.07061714e-01, 0.00000000e+00, 1.01859140e+00,\n",
       "        8.54264319e-01, 4.28344101e-01, 6.82689026e-02, 6.37374580e-01,\n",
       "        7.06429243e-01, 1.16514087e+00, 1.22970745e-01, 3.98367265e-04,\n",
       "        3.68808651e-05, 3.23511422e-01, 2.07595644e-03, 1.94215393e+00,\n",
       "        4.80371498e-04, 1.47095367e-01, 2.37077281e-01, 1.06644738e+00,\n",
       "        4.65381622e-01, 5.90208530e-01, 6.57179132e-02, 4.86911485e-05,\n",
       "        1.49107902e-04, 9.53846037e-01, 1.05026411e-05, 2.20690548e-01,\n",
       "        1.46531433e-01, 9.78508592e-01, 1.17244148e+00, 9.88206081e-03,\n",
       "        7.01445520e-01, 9.67687964e-02, 2.51283437e-01, 7.86185443e-01,\n",
       "        2.43524779e-02, 4.24792804e-02, 2.08405882e-01, 1.02428026e-01,\n",
       "        2.23260489e-04, 7.11470544e-01, 1.08731739e-01, 6.48410559e-01,\n",
       "        1.39967009e-01, 1.15517986e+00, 0.00000000e+00, 1.63854718e-01,\n",
       "        0.00000000e+00, 9.19772029e-01, 1.15741730e+00, 2.56415588e-05,\n",
       "        0.00000000e+00, 1.57368565e+00, 1.29012215e+00, 1.41118264e+00,\n",
       "        2.34370083e-02, 8.05427551e-01, 6.99286393e-05, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.73587695e-01, 9.07490492e-01, 7.56141617e-06,\n",
       "        5.94368093e-02, 3.55663598e-02, 2.74294034e-05, 6.20235026e-01,\n",
       "        7.91216850e-01, 1.94419742e-01, 0.00000000e+00, 1.09064293e+00,\n",
       "        8.97677012e-07, 1.38266611e+00, 1.80790848e-05, 1.19126484e-01,\n",
       "        1.69585383e+00, 8.80313814e-01, 2.08327528e-02, 0.00000000e+00]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.GlobalAvgPool2D()(IM(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.arange(10)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
       "  array([[0, 1, 2, 3],\n",
       "         [1, 2, 3, 4]])>,\n",
       "  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 5])>),\n",
       " (<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
       "  array([[2, 3, 4, 5],\n",
       "         [3, 4, 5, 6]])>,\n",
       "  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([6, 7])>),\n",
       " (<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
       "  array([[4, 5, 6, 7],\n",
       "         [5, 6, 7, 8]])>,\n",
       "  <tf.Tensor: shape=(2,), dtype=int32, numpy=array([8, 9])>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = keras.utils.timeseries_dataset_from_array(\n",
    "    s,\n",
    "    targets=s[4:],\n",
    "    sequence_length=4,\n",
    "    batch_size=2,\n",
    ")\n",
    "list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n",
      "[1 2 3 4]\n",
      "[2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(6).window(4, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(4))\n",
    "\n",
    "for element in dataset:\n",
    "    print(f\"{element}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_windows(dataset, length):\n",
    "    dataset = dataset.window(length, shift=1, drop_remainder=True)\n",
    "    return dataset.flat_map(lambda window: window.batch(length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
       "  array([[0, 1, 2],\n",
       "         [1, 2, 3]], dtype=int64)>,\n",
       "  <tf.Tensor: shape=(2,), dtype=int64, numpy=array([3, 4], dtype=int64)>),\n",
       " (<tf.Tensor: shape=(1, 3), dtype=int64, numpy=array([[2, 3, 4]], dtype=int64)>,\n",
       "  <tf.Tensor: shape=(1,), dtype=int64, numpy=array([5], dtype=int64)>)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = to_windows(tf.data.Dataset.range(6), 4)\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "list(dataset.batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3,), dtype=int64, numpy=array([0, 1, 2], dtype=int64)>,\n",
       " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([1, 2, 3], dtype=int64)>,\n",
       " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 3, 4], dtype=int64)>,\n",
       " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([3, 4, 5], dtype=int64)>,\n",
       " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([4, 5, 6], dtype=int64)>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = to_windows(tf.data.Dataset.range(7), 3)\n",
    "list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n",
       " array([[0, 1, 2],\n",
       "        [1, 2, 3],\n",
       "        [2, 3, 4],\n",
       "        [3, 4, 5]], dtype=int64)>,\n",
       " <tf.Tensor: shape=(4, 3), dtype=int64, numpy=\n",
       " array([[1, 2, 3],\n",
       "        [2, 3, 4],\n",
       "        [3, 4, 5],\n",
       "        [4, 5, 6]], dtype=int64)>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = to_windows(dataset, 4)\n",
    "list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 2, 3], dtype=int64)>,\n",
       "  <tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n",
       "  array([[1, 2],\n",
       "         [2, 3],\n",
       "         [3, 4],\n",
       "         [4, 5]], dtype=int64)>),\n",
       " (<tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 2, 3, 4], dtype=int64)>,\n",
       "  <tf.Tensor: shape=(4, 2), dtype=int64, numpy=\n",
       "  array([[2, 3],\n",
       "         [3, 4],\n",
       "         [4, 5],\n",
       "         [5, 6]], dtype=int64)>)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda s: (s[:, 0], s[:, 1:]))\n",
    "list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
